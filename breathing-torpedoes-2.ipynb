{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1777551,"sourceType":"datasetVersion","datasetId":1025273}],"dockerImageVersionId":30121,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install --upgrade rasterio\n! pip install -U git+https://github.com/qubvel/segmentation_models.pytorch\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-26T06:10:52.638723Z","iopub.execute_input":"2024-04-26T06:10:52.639147Z","iopub.status.idle":"2024-04-26T06:11:29.006092Z","shell.execute_reply.started":"2024-04-26T06:10:52.639056Z","shell.execute_reply":"2024-04-26T06:11:29.005040Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: rasterio in /opt/conda/lib/python3.7/site-packages (1.2.6)\nCollecting rasterio\n  Downloading rasterio-1.2.10-cp37-cp37m-manylinux1_x86_64.whl (19.3 MB)\n\u001b[K     |████████████████████████████████| 19.3 MB 11.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from rasterio) (2021.5.30)\nRequirement already satisfied: affine in /opt/conda/lib/python3.7/site-packages (from rasterio) (2.3.0)\nRequirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio) (21.2.0)\nRequirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio) (7.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio) (49.6.0.post20210108)\nRequirement already satisfied: click-plugins in /opt/conda/lib/python3.7/site-packages (from rasterio) (1.1.1)\nRequirement already satisfied: snuggs>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from rasterio) (1.4.7)\nRequirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.7/site-packages (from rasterio) (0.7.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rasterio) (1.19.5)\nRequirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\nInstalling collected packages: rasterio\n  Attempting uninstall: rasterio\n    Found existing installation: rasterio 1.2.6\n    Uninstalling rasterio-1.2.6:\n      Successfully uninstalled rasterio-1.2.6\nSuccessfully installed rasterio-1.2.10\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting git+https://github.com/qubvel/segmentation_models.pytorch\n  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-3x2ztwgi\n  Running command git clone -q https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-3x2ztwgi\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch==0.3.3) (1.15.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch==0.3.3) (4.61.1)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch==0.3.3) (0.8.1)\nCollecting efficientnet-pytorch==0.7.1\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch==0.3.3) (8.2.0)\nCollecting timm==0.9.7\n  Downloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n\u001b[K     |████████████████████████████████| 2.2 MB 14.2 MB/s eta 0:00:01\n\u001b[?25hCollecting pretrainedmodels==0.7.4\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[K     |████████████████████████████████| 58 kB 5.6 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (1.7.0)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.3.3) (2.5.0)\nCollecting safetensors\n  Downloading safetensors-0.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[K     |████████████████████████████████| 1.2 MB 55.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm==0.9.7->segmentation-models-pytorch==0.3.3) (5.4.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm==0.9.7->segmentation-models-pytorch==0.3.3) (0.0.8)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.3) (1.19.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.9.7->segmentation-models-pytorch==0.3.3) (2.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.9.7->segmentation-models-pytorch==0.3.3) (3.0.12)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.9.7->segmentation-models-pytorch==0.3.3) (3.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm==0.9.7->segmentation-models-pytorch==0.3.3) (3.4.1)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.9.7->segmentation-models-pytorch==0.3.3) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.9.7->segmentation-models-pytorch==0.3.3) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.9.7->segmentation-models-pytorch==0.3.3) (1.26.5)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.9.7->segmentation-models-pytorch==0.3.3) (2021.5.30)\nBuilding wheels for collected packages: segmentation-models-pytorch, efficientnet-pytorch, pretrainedmodels\n  Building wheel for segmentation-models-pytorch (PEP 517) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for segmentation-models-pytorch: filename=segmentation_models_pytorch-0.3.3-py3-none-any.whl size=106723 sha256=7d58dda814caedf052a2838d2cb8bf699f31a6454ac6cacb0736274ef911f02e\n  Stored in directory: /tmp/pip-ephem-wheel-cache-8fzzmzw8/wheels/fa/c5/a8/1e8af6cb04a0974db8a4a156ebd2fdd1d99ad2558d3fce49d4\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=acbc54256155913a73501a0218d5c881ccbac8f8d67ee7bfb984be1ea40162df\n  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60963 sha256=8fa7e8e9afadda39904c9459024a2c72056d8da78c30e968b38f6d9b794e03b1\n  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\nSuccessfully built segmentation-models-pytorch efficientnet-pytorch pretrainedmodels\nInstalling collected packages: safetensors, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\nSuccessfully installed efficientnet-pytorch-0.7.1 pretrainedmodels-0.7.4 safetensors-0.4.3 segmentation-models-pytorch-0.3.3 timm-0.9.7\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # image plotting\nimport matplotlib\nmatplotlib.rcParams['figure.dpi'] = 300 #increase plot resolution\n\n#Raster data handling\nfrom PIL import Image\nimport skimage\nfrom skimage import io, transform \nimport rasterio as rio # geo-oriented plotting library\nfrom rasterio import features\nimport cv2\n\n#PyTorch\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Sampler # custom dataset handling\nimport torch.autograd.profiler as profiler # to track model inference and detect leaks\nfrom torchvision import transforms, utils\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom torch.nn.modules.padding import ReplicationPad2d\nimport torchvision.models as models\nfrom torch import optim\nfrom collections import OrderedDict\nimport segmentation_models_pytorch as smp #semantic segmentation models and utils\nfrom torch.cuda.amp import GradScaler\nfrom torch.cuda.amp import autocast\n\n#Augmentations\nimport albumentations as alb\nfrom albumentations.pytorch import ToTensorV2\n\n#Logging errors and progress, sending them to tg-bot\nimport requests\nimport traceback\n\n#Other\nfrom pathlib import Path # to have path strings as PosixPath objexts\nimport pathlib \nfrom pyproj import CRS\nimport geopandas as gpd # to make dataframes out of geojson files\nimport itertools\nimport re\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport os\nimport gc\nfrom timeit import default_timer as time\nimport copy\nimport json\nimport logging\nimport datetime\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:11:29.007890Z","iopub.execute_input":"2024-04-26T06:11:29.008234Z","iopub.status.idle":"2024-04-26T06:11:35.358894Z","shell.execute_reply.started":"2024-04-26T06:11:29.008200Z","shell.execute_reply":"2024-04-26T06:11:35.357941Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from descartes import PolygonPatch","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:11:35.360909Z","iopub.execute_input":"2024-04-26T06:11:35.361263Z","iopub.status.idle":"2024-04-26T06:11:35.367574Z","shell.execute_reply.started":"2024-04-26T06:11:35.361206Z","shell.execute_reply":"2024-04-26T06:11:35.366864Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:11:35.368982Z","iopub.execute_input":"2024-04-26T06:11:35.369299Z","iopub.status.idle":"2024-04-26T06:11:35.377556Z","shell.execute_reply.started":"2024-04-26T06:11:35.369251Z","shell.execute_reply":"2024-04-26T06:11:35.376671Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:11:35.378706Z","iopub.execute_input":"2024-04-26T06:11:35.378973Z","iopub.status.idle":"2024-04-26T06:11:35.567075Z","shell.execute_reply.started":"2024-04-26T06:11:35.378947Z","shell.execute_reply":"2024-04-26T06:11:35.566134Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"66"},"metadata":{}}]},{"cell_type":"code","source":"root_folder = '../input/spacenet-7-change-detection-chips-and-masks/chip_dataset/chip_dataset/change_detection/'\ncsv_path = '../input/spacenet-7-change-detection-chips-and-masks/annotations.csv'\n\nBATCH_SIZE = 64\nNUM_WORKERS = 8","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:11:35.568386Z","iopub.execute_input":"2024-04-26T06:11:35.568673Z","iopub.status.idle":"2024-04-26T06:11:35.576239Z","shell.execute_reply.started":"2024-04-26T06:11:35.568632Z","shell.execute_reply":"2024-04-26T06:11:35.575367Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(csv_path)\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:11:35.577292Z","iopub.execute_input":"2024-04-26T06:11:35.577542Z","iopub.status.idle":"2024-04-26T06:12:09.460570Z","shell.execute_reply.started":"2024-04-26T06:11:35.577517Z","shell.execute_reply":"2024-04-26T06:12:09.459655Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                 chip_path  \\\n492308   L15-0331E-1257N_1327_3160_13/chips/2018_4_2018...   \n1092628  L15-1669E-1160N_6679_3549_13/chips/2018_12_201...   \n2583636  L15-0457E-1135N_1831_3648_13/chips/2019_5_2019...   \n232249   L15-1200E-0847N_4802_4803_13/chips/2018_10_201...   \n2467193  L15-0358E-1220N_1433_3310_13/chips/2018_2_2019...   \n\n                                                 mask_path  target  \\\n492308   L15-0331E-1257N_1327_3160_13/masks/2018_4_2018...       0   \n1092628  L15-1669E-1160N_6679_3549_13/masks/2018_12_201...       0   \n2583636  L15-0457E-1135N_1831_3648_13/masks/2019_5_2019...       0   \n232249   L15-1200E-0847N_4802_4803_13/masks/2018_10_201...       0   \n2467193  L15-0358E-1220N_1433_3310_13/masks/2018_2_2019...       1   \n\n                                                     fname        im_dates  \\\n492308   global_monthly_2018_4_2018_2_chip_x768_y832_L1...   2018_4_2018_2   \n1092628  global_monthly_2018_12_2019_3_chip_x384_y640_L...  2018_12_2019_3   \n2583636  global_monthly_2019_5_2019_10_chip_x384_y64_L1...  2019_5_2019_10   \n232249   global_monthly_2018_10_2019_7_chip_x832_y384_L...  2018_10_2019_7   \n2467193  global_monthly_2018_2_2019_10_chip_x384_y704_L...  2018_2_2019_10   \n\n         year1  month1  year2  month2    x    y                       im_name  \\\n492308    2018       4   2018       2  768  832  L15-0331E-1257N_1327_3160_13   \n1092628   2018      12   2019       3  384  640  L15-1669E-1160N_6679_3549_13   \n2583636   2019       5   2019      10  384   64  L15-0457E-1135N_1831_3648_13   \n232249    2018      10   2019       7  832  384  L15-1200E-0847N_4802_4803_13   \n2467193   2018       2   2019      10  384  704  L15-0358E-1220N_1433_3310_13   \n\n        is_blank  \n492308     blank  \n1092628    blank  \n2583636    blank  \n232249     blank  \n2467193      NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chip_path</th>\n      <th>mask_path</th>\n      <th>target</th>\n      <th>fname</th>\n      <th>im_dates</th>\n      <th>year1</th>\n      <th>month1</th>\n      <th>year2</th>\n      <th>month2</th>\n      <th>x</th>\n      <th>y</th>\n      <th>im_name</th>\n      <th>is_blank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>492308</th>\n      <td>L15-0331E-1257N_1327_3160_13/chips/2018_4_2018...</td>\n      <td>L15-0331E-1257N_1327_3160_13/masks/2018_4_2018...</td>\n      <td>0</td>\n      <td>global_monthly_2018_4_2018_2_chip_x768_y832_L1...</td>\n      <td>2018_4_2018_2</td>\n      <td>2018</td>\n      <td>4</td>\n      <td>2018</td>\n      <td>2</td>\n      <td>768</td>\n      <td>832</td>\n      <td>L15-0331E-1257N_1327_3160_13</td>\n      <td>blank</td>\n    </tr>\n    <tr>\n      <th>1092628</th>\n      <td>L15-1669E-1160N_6679_3549_13/chips/2018_12_201...</td>\n      <td>L15-1669E-1160N_6679_3549_13/masks/2018_12_201...</td>\n      <td>0</td>\n      <td>global_monthly_2018_12_2019_3_chip_x384_y640_L...</td>\n      <td>2018_12_2019_3</td>\n      <td>2018</td>\n      <td>12</td>\n      <td>2019</td>\n      <td>3</td>\n      <td>384</td>\n      <td>640</td>\n      <td>L15-1669E-1160N_6679_3549_13</td>\n      <td>blank</td>\n    </tr>\n    <tr>\n      <th>2583636</th>\n      <td>L15-0457E-1135N_1831_3648_13/chips/2019_5_2019...</td>\n      <td>L15-0457E-1135N_1831_3648_13/masks/2019_5_2019...</td>\n      <td>0</td>\n      <td>global_monthly_2019_5_2019_10_chip_x384_y64_L1...</td>\n      <td>2019_5_2019_10</td>\n      <td>2019</td>\n      <td>5</td>\n      <td>2019</td>\n      <td>10</td>\n      <td>384</td>\n      <td>64</td>\n      <td>L15-0457E-1135N_1831_3648_13</td>\n      <td>blank</td>\n    </tr>\n    <tr>\n      <th>232249</th>\n      <td>L15-1200E-0847N_4802_4803_13/chips/2018_10_201...</td>\n      <td>L15-1200E-0847N_4802_4803_13/masks/2018_10_201...</td>\n      <td>0</td>\n      <td>global_monthly_2018_10_2019_7_chip_x832_y384_L...</td>\n      <td>2018_10_2019_7</td>\n      <td>2018</td>\n      <td>10</td>\n      <td>2019</td>\n      <td>7</td>\n      <td>832</td>\n      <td>384</td>\n      <td>L15-1200E-0847N_4802_4803_13</td>\n      <td>blank</td>\n    </tr>\n    <tr>\n      <th>2467193</th>\n      <td>L15-0358E-1220N_1433_3310_13/chips/2018_2_2019...</td>\n      <td>L15-0358E-1220N_1433_3310_13/masks/2018_2_2019...</td>\n      <td>1</td>\n      <td>global_monthly_2018_2_2019_10_chip_x384_y704_L...</td>\n      <td>2018_2_2019_10</td>\n      <td>2018</td>\n      <td>2</td>\n      <td>2019</td>\n      <td>10</td>\n      <td>384</td>\n      <td>704</td>\n      <td>L15-0358E-1220N_1433_3310_13</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.target.mean()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:12:09.463029Z","iopub.execute_input":"2024-04-26T06:12:09.463304Z","iopub.status.idle":"2024-04-26T06:12:09.474321Z","shell.execute_reply.started":"2024-04-26T06:12:09.463277Z","shell.execute_reply":"2024-04-26T06:12:09.473509Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"0.17641241530490234"},"metadata":{}}]},{"cell_type":"code","source":"df[df.target == 0].is_blank.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:12:09.476101Z","iopub.execute_input":"2024-04-26T06:12:09.476374Z","iopub.status.idle":"2024-04-26T06:12:10.671800Z","shell.execute_reply.started":"2024-04-26T06:12:09.476346Z","shell.execute_reply":"2024-04-26T06:12:10.670904Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"blank    2644968\nName: is_blank, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"aoi = df['im_name'].unique()\nlen(aoi)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:12:10.672857Z","iopub.execute_input":"2024-04-26T06:12:10.673113Z","iopub.status.idle":"2024-04-26T06:12:11.014999Z","shell.execute_reply.started":"2024-04-26T06:12:10.673087Z","shell.execute_reply":"2024-04-26T06:12:11.014081Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"60"},"metadata":{}}]},{"cell_type":"code","source":"train_aoi = aoi[:40]\ntest_aoi = aoi[-20:-10]\nvalid_aoi = aoi[-10:]","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:12:11.016239Z","iopub.execute_input":"2024-04-26T06:12:11.016529Z","iopub.status.idle":"2024-04-26T06:12:11.020537Z","shell.execute_reply.started":"2024-04-26T06:12:11.016500Z","shell.execute_reply":"2024-04-26T06:12:11.019592Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def choose_aoi(df, names):\n    mask = df['im_name'].map(lambda x: x in names)\n    return df[mask].reset_index(drop=True)\n\ndf_dict = {'train' : choose_aoi(df, train_aoi),\n          'test' : choose_aoi(df, test_aoi),\n          'valid' : choose_aoi(df, valid_aoi)\n          }\n\ndel(df)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:12:11.021969Z","iopub.execute_input":"2024-04-26T06:12:11.022413Z","iopub.status.idle":"2024-04-26T06:13:37.262554Z","shell.execute_reply.started":"2024-04-26T06:12:11.022361Z","shell.execute_reply":"2024-04-26T06:13:37.261599Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class TorchDataset(Dataset):\n    \"\"\"Dataset class\n    Args:\n        root_folder: Path object, root directory of picture dataset\n        csv: pandas.DataFrame, untidy df with all data relationships\n        aug: albumentations dictionary\n        preproc: callable, preprocessing function related to specific encoder\n        grayscale: boolean, preprocessing condition to grayscale colored rasters\n    Return:\n        image, mask tensors\"\"\"\n    \n    def __init__(self, root_folder, df, aug = None, preproc = None, grayscale = True):\n        self.root_folder = root_folder\n        self.csv = df\n        self.aug = aug\n        self.preproc = preproc\n        self.grayscale = grayscale\n    \n    def __len__(self):\n        return len(self.csv)\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        chip_path = self.root_folder + self.csv.loc[idx,'chip_path']\n        # read chip into numpy array\n        chip = skimage.io.imread(root_folder + self.csv.loc[idx,'chip_path']).astype('float32')\n        if self.grayscale:\n            gray1 = np.dot(chip[:,:,0:3], [0.2989, 0.5870, 0.1140])\n            gray2 = np.dot(chip[:,:,3:], [0.2989, 0.5870, 0.1140])\n            chip = np.divide(np.stack((gray1, gray2),axis = 2),255).astype('float32')\n        # get target for corresponding chip\n        mask = np.abs(np.divide(skimage.io.imread(root_folder + self.csv.loc[idx,'mask_path']),255)).astype('float32')\n        # apply augmentations\n        if self.aug:\n            sample = self.aug(image=chip, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            mask = mask.unsqueeze(0)\n            if self.grayscale:\n                sample = {'I1':image[0,:,:].unsqueeze(0),'I2':image[1,:,:].unsqueeze(0), 'label':mask}\n            else: \n                sample = {'I1':image[0:3,:,:],'I2':image[3:,:,:], 'label':mask}\n            del(image,mask,chip,gray1,gray2)\n            return sample\n        else:\n            image = torch.Tensor(np.moveaxis(chip, 2, 0))\n            mask = torch.Tensor(mask).unsqueeze(0)\n            if self.grayscale:\n                sample = {'I1':image[0,:,:].unsqueeze(0),'I2':image[1,:,:].unsqueeze(0), 'label':mask}\n            else: \n                sample = {'I1':image[0:3,:,:],'I2':image[3:,:,:], 'label':mask}\n            del(mask,chip,gray1,gray2)\n            return sample\n    \n\n    \n    \n\n\nclass BalancedSampler(Sampler):\n    \"\"\"Balancer for torch.DataLoader to adjust chips loading\"\"\"\n    \n    def __init__(self, dataset, percentage = 0.5):\n        \"\"\"\n        dataset: custom torch dataset\n        percentage: float number between 0 and 1, percentage of change containing pictures in batch\n        \"\"\"\n        assert 0 <= percentage <= 1,'percentage must be a value between 0 and 1'\n        \n        self.dataset = dataset\n        self.pct = percentage\n        self.len_ = len(dataset)\n    \n    def __len__(self):\n        return self.len_\n    \n    def __iter__(self):\n        # get indices for chips containing change and blank ones\n        change_chip_idxs = np.where(self.dataset.csv['target'] == 1)[0]\n        blank_chip_idxs = np.where(self.dataset.csv['target'] == 0)[0]\n        # randomly sample from the incides of each class according to percentage value\n        change_chip_idxs = np.random.choice(change_chip_idxs,int(self.len_ * self.pct), replace=True)\n        blank_chip_idxs = np.random.choice(blank_chip_idxs,int(self.len_ * (1 - self.pct))+1, replace=False)\n        # stack and shuffle of sampled indices\n        all_idxs = np.hstack([change_chip_idxs,blank_chip_idxs])\n        np.random.shuffle(all_idxs)\n        return iter(all_idxs)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:13:37.264037Z","iopub.execute_input":"2024-04-26T06:13:37.264416Z","iopub.status.idle":"2024-04-26T06:13:37.289290Z","shell.execute_reply.started":"2024-04-26T06:13:37.264381Z","shell.execute_reply":"2024-04-26T06:13:37.288082Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"chip_dimension = 64\naugs = {\n    'train': alb.Compose([\n        alb.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n        alb.HorizontalFlip(p=0.5),\n        alb.VerticalFlip(p=0.5),\n        ToTensorV2() #apparently doesn't work properly with smp Unet, included in get_preprocessing function\n    ]),\n    'test': alb.Compose([\n        alb.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n        ToTensorV2()\n    ]), \n    'valid': alb.Compose([\n        alb.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n        ToTensorV2()\n    ]),\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:13:37.290814Z","iopub.execute_input":"2024-04-26T06:13:37.291262Z","iopub.status.idle":"2024-04-26T06:13:37.304764Z","shell.execute_reply.started":"2024-04-26T06:13:37.291218Z","shell.execute_reply":"2024-04-26T06:13:37.303871Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"augs['train']","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:13:37.305917Z","iopub.execute_input":"2024-04-26T06:13:37.306269Z","iopub.status.idle":"2024-04-26T06:13:37.323876Z","shell.execute_reply.started":"2024-04-26T06:13:37.306229Z","shell.execute_reply":"2024-04-26T06:13:37.322967Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Compose([\n  PadIfNeeded(always_apply=False, p=1, min_height=64, min_width=64, pad_height_divisor=None, pad_width_divisor=None, border_mode=4, value=0, mask_value=None),\n  HorizontalFlip(always_apply=False, p=0.5),\n  VerticalFlip(always_apply=False, p=0.5),\n  ToTensorV2(always_apply=True, p=1.0, transpose_mask=False),\n], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nclass IoULoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(IoULoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        #inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #intersection is equivalent to True Positive count\n        #union is the mutually inclusive area of all labels & predictions \n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection \n        \n        IoU = (intersection + smooth)/(union + smooth)\n                \n        return 1 - IoU\n\ndef iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n    \"\"\"Fast enough iou calculation function\"\"\"\n    SMOOTH = 1e-6\n    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n    #outputs = outputs.detach()\n    #labels = labels.detach()\n    \n    intersection = (outputs & labels).float().sum((1, 2))\n    union = (outputs | labels).float().sum((1, 2))\n    \n    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n    \n    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n    \n    return thresholded.mean() # to get a batch average\n\ndef segmentation_report(running_preds, running_labels):\n    \"\"\"Function to get a closer look to a confusion metrics and related metrics\"\"\"\n    rp = running_preds.flatten()\n    rl = running_labels.flatten()\n    tn, fp, fn, tp = confusion_matrix(rl, rp, labels=[0,1]).ravel()\n    px_accuracy = (tp+tn) / (tp+fp+tn+fn)\n    precision = tp / (tp+fp)\n    recall = tp / (tp+fn)\n    #calculating intersection over union\n    intersection = np.logical_and(rl, rp)\n    union = np.logical_or(rl, rp)\n    iou_score = np.sum(intersection) / np.sum(union)\n    fmeasure = 2 * precision * recall / (precision + recall)\n    #making report\n    report = { 'tp/tn/fp/fn' : (tp,tn,fp,fn),\n              'px_accuracy': px_accuracy,\n              'precision': precision,\n              'recall': recall,\n              'iou_score': iou_score,\n              'fmeasure':fmeasure\n             }\n    return report\n\n\ndef log_batch_statistics(batch_number,batch_labels,batch_preds,phase,loss,since,num_batches,period=500):\n    if batch_number % period == 0:\n        iou_score = segmentation_report(batch_preds,batch_labels)\n        time_elapsed = time.time() - since\n\n        if phase == 'train':\n            telegram_bot_sendtext('TRAINING BATCH')\n        else:\n            telegram_bot_sendtext('VALIDATION BATCH')\n            \n        telegram_bot_sendtext('-'*50)\n        telegram_bot_sendtext(f'\\n{batch_number}/{num_batches-1}:')\n        telegram_bot_sendtext(f'Total Time Elapsed: {time_elapsed/60:.2f} mins')\n        telegram_bot_sendtext(f'Batch Loss: {loss.item():.4f}\\n')\n        telegram_bot_sendtext(f\"``` IoU_score:{iou_score}\\n ```\")\n        telegram_bot_sendtext('-'*50)\n\ndef break_time_limit(start_time,time_limit=28080):\n    time_elapsed = time()-start_time\n    if time_elapsed > time_limit:\n        sys.exit()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:13:37.324958Z","iopub.execute_input":"2024-04-26T06:13:37.325251Z","iopub.status.idle":"2024-04-26T06:13:37.473454Z","shell.execute_reply.started":"2024-04-26T06:13:37.325224Z","shell.execute_reply":"2024-04-26T06:13:37.472581Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#learning policy params\ngrayscale = True\nif grayscale == True:\n    in_channels = 2\nelse: in_channels = 6\n\nN_EPOCHS = 25\n\n# turning on GPU if possible\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n# cleaning GPU\ngc.collect() \ntorch.cuda.empty_cache()\ntorch.backends.cudnn.benchmark = True\n\n# mean percentage of positives is 6.5% from the frame, median is 3.4%, so weights for bce loss are required.\nweights = torch.Tensor([28]).to(device) \n\n#criterion = torch.nn.BCEWithLogitsLoss(pos_weight = weights) \ncriterion = IoULoss()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:13:37.474619Z","iopub.execute_input":"2024-04-26T06:13:37.474922Z","iopub.status.idle":"2024-04-26T06:13:42.047494Z","shell.execute_reply.started":"2024-04-26T06:13:37.474891Z","shell.execute_reply":"2024-04-26T06:13:42.046741Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Using device: cuda:0\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Change detection FC model","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass Unet(nn.Module):\n    \"\"\"EF segmentation network.\"\"\"\n\n    def __init__(self, input_nbr, label_nbr):\n        super(Unet, self).__init__()\n\n        self.input_nbr = input_nbr\n\n        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n        self.bn11 = nn.BatchNorm2d(16)\n        self.do11 = nn.Dropout2d(p=0.2)\n        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n        self.bn12 = nn.BatchNorm2d(16)\n        self.do12 = nn.Dropout2d(p=0.2)\n\n        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.bn21 = nn.BatchNorm2d(32)\n        self.do21 = nn.Dropout2d(p=0.2)\n        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n        self.bn22 = nn.BatchNorm2d(32)\n        self.do22 = nn.Dropout2d(p=0.2)\n\n        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn31 = nn.BatchNorm2d(64)\n        self.do31 = nn.Dropout2d(p=0.2)\n        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn32 = nn.BatchNorm2d(64)\n        self.do32 = nn.Dropout2d(p=0.2)\n        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn33 = nn.BatchNorm2d(64)\n        self.do33 = nn.Dropout2d(p=0.2)\n\n        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn41 = nn.BatchNorm2d(128)\n        self.do41 = nn.Dropout2d(p=0.2)\n        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn42 = nn.BatchNorm2d(128)\n        self.do42 = nn.Dropout2d(p=0.2)\n        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn43 = nn.BatchNorm2d(128)\n        self.do43 = nn.Dropout2d(p=0.2)\n\n\n        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n        self.bn43d = nn.BatchNorm2d(128)\n        self.do43d = nn.Dropout2d(p=0.2)\n        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n        self.bn42d = nn.BatchNorm2d(128)\n        self.do42d = nn.Dropout2d(p=0.2)\n        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn41d = nn.BatchNorm2d(64)\n        self.do41d = nn.Dropout2d(p=0.2)\n\n        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn33d = nn.BatchNorm2d(64)\n        self.do33d = nn.Dropout2d(p=0.2)\n        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n        self.bn32d = nn.BatchNorm2d(64)\n        self.do32d = nn.Dropout2d(p=0.2)\n        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn31d = nn.BatchNorm2d(32)\n        self.do31d = nn.Dropout2d(p=0.2)\n\n        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn22d = nn.BatchNorm2d(32)\n        self.do22d = nn.Dropout2d(p=0.2)\n        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn21d = nn.BatchNorm2d(16)\n        self.do21d = nn.Dropout2d(p=0.2)\n\n        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn12d = nn.BatchNorm2d(16)\n        self.do12d = nn.Dropout2d(p=0.2)\n        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n\n        self.sm = nn.Sigmoid()\n\n    def forward(self, x1, x2):\n\n        x = torch.cat((x1, x2), 1)\n\n        \"\"\"Forward method.\"\"\"\n        # Stage 1\n        x11 = self.do11(F.relu(self.bn11(self.conv11(x))))\n        x12 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n        x1p = F.max_pool2d(x12, kernel_size=2, stride=2)\n\n        # Stage 2\n        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n        x22 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n        x2p = F.max_pool2d(x22, kernel_size=2, stride=2)\n\n        # Stage 3\n        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n        x33 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n        x3p = F.max_pool2d(x33, kernel_size=2, stride=2)\n\n        # Stage 4\n        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n        x43 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n        x4p = F.max_pool2d(x43, kernel_size=2, stride=2)\n\n\n        # Stage 4d\n        x4d = self.upconv4(x4p)\n        pad4 = ReplicationPad2d((0, x43.size(3) - x4d.size(3), 0, x43.size(2) - x4d.size(2)))\n        x4d = torch.cat((pad4(x4d), x43), 1)\n        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n\n        # Stage 3d\n        x3d = self.upconv3(x41d)\n        pad3 = ReplicationPad2d((0, x33.size(3) - x3d.size(3), 0, x33.size(2) - x3d.size(2)))\n        x3d = torch.cat((pad3(x3d), x33), 1)\n        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n\n        # Stage 2d\n        x2d = self.upconv2(x31d)\n        pad2 = ReplicationPad2d((0, x22.size(3) - x2d.size(3), 0, x22.size(2) - x2d.size(2)))\n        x2d = torch.cat((pad2(x2d), x22), 1)\n        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n\n        # Stage 1d\n        x1d = self.upconv1(x21d)\n        pad1 = ReplicationPad2d((0, x12.size(3) - x1d.size(3), 0, x12.size(2) - x1d.size(2)))\n        x1d = torch.cat((pad1(x1d), x12), 1)\n        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n        x11d = self.conv11d(x12d)\n\n        return self.sm(x11d)\n\nclass SiamUnet_diff(nn.Module):\n    \"\"\"SiamUnet_diff segmentation network.\"\"\"\n\n    def __init__(self, input_nbr, label_nbr):\n        super(SiamUnet_diff, self).__init__()\n\n        self.input_nbr = input_nbr\n\n        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n        self.bn11 = nn.BatchNorm2d(16)\n        self.do11 = nn.Dropout2d(p=0.2)\n        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n        self.bn12 = nn.BatchNorm2d(16)\n        self.do12 = nn.Dropout2d(p=0.2)\n\n        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.bn21 = nn.BatchNorm2d(32)\n        self.do21 = nn.Dropout2d(p=0.2)\n        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n        self.bn22 = nn.BatchNorm2d(32)\n        self.do22 = nn.Dropout2d(p=0.2)\n\n        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn31 = nn.BatchNorm2d(64)\n        self.do31 = nn.Dropout2d(p=0.2)\n        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn32 = nn.BatchNorm2d(64)\n        self.do32 = nn.Dropout2d(p=0.2)\n        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn33 = nn.BatchNorm2d(64)\n        self.do33 = nn.Dropout2d(p=0.2)\n\n        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn41 = nn.BatchNorm2d(128)\n        self.do41 = nn.Dropout2d(p=0.2)\n        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn42 = nn.BatchNorm2d(128)\n        self.do42 = nn.Dropout2d(p=0.2)\n        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn43 = nn.BatchNorm2d(128)\n        self.do43 = nn.Dropout2d(p=0.2)\n\n        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n        self.bn43d = nn.BatchNorm2d(128)\n        self.do43d = nn.Dropout2d(p=0.2)\n        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n        self.bn42d = nn.BatchNorm2d(128)\n        self.do42d = nn.Dropout2d(p=0.2)\n        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn41d = nn.BatchNorm2d(64)\n        self.do41d = nn.Dropout2d(p=0.2)\n\n        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn33d = nn.BatchNorm2d(64)\n        self.do33d = nn.Dropout2d(p=0.2)\n        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n        self.bn32d = nn.BatchNorm2d(64)\n        self.do32d = nn.Dropout2d(p=0.2)\n        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn31d = nn.BatchNorm2d(32)\n        self.do31d = nn.Dropout2d(p=0.2)\n\n        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn22d = nn.BatchNorm2d(32)\n        self.do22d = nn.Dropout2d(p=0.2)\n        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn21d = nn.BatchNorm2d(16)\n        self.do21d = nn.Dropout2d(p=0.2)\n\n        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn12d = nn.BatchNorm2d(16)\n        self.do12d = nn.Dropout2d(p=0.2)\n        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n\n        self.sm = nn.Sigmoid()\n\n    def forward(self, x1, x2):\n\n\n        \"\"\"Forward method.\"\"\"\n        # Stage 1\n        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n\n\n        # Stage 2\n        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n\n        # Stage 3\n        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n\n        # Stage 4\n        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n\n        ####################################################\n        # Stage 1\n        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n\n\n        # Stage 2\n        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n\n        # Stage 3\n        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n\n        # Stage 4\n        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n\n\n\n        # Stage 4d\n        x4d = self.upconv4(x4p)\n        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n        x4d = torch.cat((pad4(x4d), torch.abs(x43_1 - x43_2)), 1)\n        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n\n        # Stage 3d\n        x3d = self.upconv3(x41d)\n        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n        x3d = torch.cat((pad3(x3d), torch.abs(x33_1 - x33_2)), 1)\n        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n\n        # Stage 2d\n        x2d = self.upconv2(x31d)\n        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n        x2d = torch.cat((pad2(x2d), torch.abs(x22_1 - x22_2)), 1)\n        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n\n        # Stage 1d\n        x1d = self.upconv1(x21d)\n        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n        x1d = torch.cat((pad1(x1d), torch.abs(x12_1 - x12_2)), 1)\n        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n        x11d = self.conv11d(x12d)\n\n        return self.sm(x11d)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:13:42.048990Z","iopub.execute_input":"2024-04-26T06:13:42.049401Z","iopub.status.idle":"2024-04-26T06:13:42.142618Z","shell.execute_reply.started":"2024-04-26T06:13:42.049359Z","shell.execute_reply":"2024-04-26T06:13:42.141686Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"!pip install segmentation_models_pytorch\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:13:42.143868Z","iopub.execute_input":"2024-04-26T06:13:42.144188Z","iopub.status.idle":"2024-04-26T06:13:49.879970Z","shell.execute_reply.started":"2024-04-26T06:13:42.144133Z","shell.execute_reply":"2024-04-26T06:13:49.878849Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Requirement already satisfied: segmentation_models_pytorch in /opt/conda/lib/python3.7/site-packages (0.3.3)\nRequirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.7.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (1.15.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (4.61.1)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.7.4)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (8.2.0)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.8.1)\nRequirement already satisfied: timm==0.9.7 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.9.7)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.7.0)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (2.5.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm==0.9.7->segmentation_models_pytorch) (5.4.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm==0.9.7->segmentation_models_pytorch) (0.0.8)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.7/site-packages (from timm==0.9.7->segmentation_models_pytorch) (0.4.3)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.19.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.9.7->segmentation_models_pytorch) (2.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.9.7->segmentation_models_pytorch) (3.0.12)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm==0.9.7->segmentation_models_pytorch) (3.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm==0.9.7->segmentation_models_pytorch) (3.4.1)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.9.7->segmentation_models_pytorch) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.9.7->segmentation_models_pytorch) (1.26.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.9.7->segmentation_models_pytorch) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm==0.9.7->segmentation_models_pytorch) (2021.5.30)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"if grayscale == True:\n    #net, net_name = smp.Unet('resnet34', in_channels = 2, activation='sigmoid'), 'FC-EF'\n    net, net_name = SiamUnet_diff(1,1), 'FC-Siam-diff'\n    #net, net_name = SiamUnet_conc(1,1), 'FC-Siam-conc'\n    #net, net_name = Unet(2,1), 'FC-EF'\n    \nelse:\n    #net, net_name = smp.Unet('resnet34', in_channels = 6, activation='sigmoid'), 'FC-EF'\n    net, net_name = SiamUnet_diff(3, 1), 'FC-Siam-diff'\n    #net, net_name = SiamUnet_conc(3,1), 'FC-Siam-conc'\n    #net, net_name = Unet(3*2,1), 'FC-EF'\nnet = net.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:13:49.881680Z","iopub.execute_input":"2024-04-26T06:13:49.881988Z","iopub.status.idle":"2024-04-26T06:13:49.922279Z","shell.execute_reply.started":"2024-04-26T06:13:49.881956Z","shell.execute_reply":"2024-04-26T06:13:49.921415Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"net","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:13:49.923712Z","iopub.execute_input":"2024-04-26T06:13:49.924143Z","iopub.status.idle":"2024-04-26T06:13:49.931492Z","shell.execute_reply.started":"2024-04-26T06:13:49.924084Z","shell.execute_reply":"2024-04-26T06:13:49.930536Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"SiamUnet_diff(\n  (conv11): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do11): Dropout2d(p=0.2, inplace=False)\n  (conv12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn12): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do12): Dropout2d(p=0.2, inplace=False)\n  (conv21): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do21): Dropout2d(p=0.2, inplace=False)\n  (conv22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn22): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do22): Dropout2d(p=0.2, inplace=False)\n  (conv31): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do31): Dropout2d(p=0.2, inplace=False)\n  (conv32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn32): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do32): Dropout2d(p=0.2, inplace=False)\n  (conv33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do33): Dropout2d(p=0.2, inplace=False)\n  (conv41): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do41): Dropout2d(p=0.2, inplace=False)\n  (conv42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn42): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do42): Dropout2d(p=0.2, inplace=False)\n  (conv43): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do43): Dropout2d(p=0.2, inplace=False)\n  (upconv4): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n  (conv43d): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn43d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do43d): Dropout2d(p=0.2, inplace=False)\n  (conv42d): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn42d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do42d): Dropout2d(p=0.2, inplace=False)\n  (conv41d): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn41d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do41d): Dropout2d(p=0.2, inplace=False)\n  (upconv3): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n  (conv33d): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn33d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do33d): Dropout2d(p=0.2, inplace=False)\n  (conv32d): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn32d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do32d): Dropout2d(p=0.2, inplace=False)\n  (conv31d): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn31d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do31d): Dropout2d(p=0.2, inplace=False)\n  (upconv2): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n  (conv22d): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn22d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do22d): Dropout2d(p=0.2, inplace=False)\n  (conv21d): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn21d): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do21d): Dropout2d(p=0.2, inplace=False)\n  (upconv1): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n  (conv12d): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn12d): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (do12d): Dropout2d(p=0.2, inplace=False)\n  (conv11d): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (sm): Sigmoid()\n)"},"metadata":{}}]},{"cell_type":"code","source":"#defining datasets, samplers and dataloaders\ndatasets = {x:TorchDataset(root_folder = root_folder,df = df_dict[x],aug = None, preproc = None, grayscale = grayscale) for x in ['train','test','valid']}\n\nsamplers = {'train':BalancedSampler(datasets['train'], percentage = 1), \n            'test':BalancedSampler(datasets['test'], percentage = 1),\n            'valid':BalancedSampler(datasets['valid'], percentage = 1),\n            'sanity':BalancedSampler(datasets['train'], percentage = 1)}\n\ndataloaders = {x: DataLoader(dataset=datasets[x],\n                             batch_size=BATCH_SIZE,\n                             sampler=samplers[x],\n                             num_workers=16) for x in ['train','test','valid']}\n\ndataset_sizes = {x: len(datasets[x]) for x in ['train', 'test', 'valid']}\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:13:49.932740Z","iopub.execute_input":"2024-04-26T06:13:49.933061Z","iopub.status.idle":"2024-04-26T06:13:49.944134Z","shell.execute_reply.started":"2024-04-26T06:13:49.933033Z","shell.execute_reply":"2024-04-26T06:13:49.943201Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def train(net, n_epochs = 25):\n    print('epoch,train_loss,train_iou,test_loss,test_iou',file=open('loss_log.txt', 'a'))\n    start_time = time()\n    # telegram_bot_sendtext(f'Training started')\n    scaler = GradScaler()\n        \n    iou = -1\n    best_iou = -1\n    \n    lss = 100000000\n    best_lss = 100000000\n    \n    train_epoch_iou = 0\n    test_epoch_iou = 0\n    \n    \n    #defining optimizer and scheduler\n    optimizer = torch.optim.Adam(net.parameters(), lr = 0.005)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n    \n\n    for epoch_index in tqdm(range(n_epochs)):\n        print('Epoch: ' + str(epoch_index + 1) + ' of ' + str(n_epochs))\n        train_running_loss = 0.0\n        train_running_iou = 0.0\n        test_running_loss = 0.0\n        test_running_iou = 0.0\n        \n        for phase in ['train','test']:\n            if phase == 'train':\n                net.train()  # Set model to training mode\n            else:\n                net.eval()   # Set model to evaluate mode\n\n            num_batches = len(dataloaders[phase])\n            for batch_index, batch in enumerate(tqdm(dataloaders[phase])):\n                torch.cuda.empty_cache()\n                I1 = Variable(batch['I1'].float().to(device))\n                I2 = Variable(batch['I2'].float().to(device))\n                labels = Variable(batch['label'].float().to(device))\n\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == 'train'):\n                    with autocast():\n                        outputs = net(I1, I2)\n                        #outputs = net(torch.stack((I1, I2), axis = 1).squeeze()) # fusion for smp.Unet\n                        loss = criterion(outputs, labels)\n                    _, preds = torch.max(outputs.data, 1)\n                    del(_,outputs)\n                    if phase == 'train':\n                        scaler.scale(loss).backward()\n                        scaler.step(optimizer)\n                        optimizer.step()\n                        scaler.update()\n                    del(I1, I2)\n#                     except Exception as e:\n#                             msg = log_traceback(e)\n#                             telegram_bot_sendtext(f'failed at batch {batch_index}, with message: {msg}')\n#                             raise e\n#                             break\n#                     log_batch_statistics(batch_index,labels.data.long(),outputs.data.long(),phase,loss=loss,num_batches=num_batches,since=start_time)\n                if phase == 'train':\n                    train_running_loss += loss.item()\n                    # train_running_iou += iou_pytorch(preds.int().to('cpu'), labels.int().to('cpu')).item()\n                else:\n                    test_running_loss += loss.item()\n                    # test_running_iou += iou_pytorch(preds.int().to('cpu'), labels.int().to('cpu')).item()\n                                \n            if phase == 'train':\n                scheduler.step()\n                train_epoch_loss = train_running_loss / dataset_sizes[phase] * BATCH_SIZE\n                # train_epoch_iou = train_running_iou / dataset_sizes[phase] * BATCH_SIZE\n            else:\n                test_epoch_loss = test_running_loss / dataset_sizes[phase] * BATCH_SIZE\n                # test_epoch_iou = test_running_iou / dataset_sizes[phase] * BATCH_SIZE\n            #print(f'{epoch_index},{train_epoch_loss}, {train_epoch_iou},{test_epoch_loss},{test_epoch_iou}', file=open('loss_log.txt', 'a'))\n#             print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=20), file=open(\"profiler.txt\", \"a\"))\n#             telegram_send_file('./profiler.txt')\n            '''\n            if epoch_iou > best_iou:\n                best_iou = epoch_iou\n                save_str = f'./net{net_name}-best_epoch-' + str(epoch_index + 1) + '_iou_score-' + str(best_iou) + '.pth.tar'\n                torch.save(net, save_str)\n#                 telegram_send_file(save_str)\n                print('Model saved!')\n            '''\n            if (phase  == 'train') and (train_epoch_loss < best_lss):\n                best_lss = train_epoch_loss\n                save_str = f'./net{net_name}-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(best_lss) + '.pth.tar'\n                torch.save(net, save_str)\n                #telegram_send_file(save_str)\n                print('Model saved!')\n            \n            if (phase == 'test') and (test_epoch_loss < best_lss):\n                best_lss = test_epoch_loss\n                save_str = f'./net{net_name}-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(best_lss) + '.pth.tar'\n                torch.save(net, save_str)\n                #telegram_send_file(save_str)\n                print('Model saved!')\n        \n    \n    model_str = './model.pth.tar'\n    torch.save(net, model_str)\n    #telegram_send_file(model_str)\n    print('Model saved!')\n    time_elapsed = time() - start_time\n    print()\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val change_accuracy: {best_fm:4f}')\n    \n    return net\n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:13:49.947827Z","iopub.execute_input":"2024-04-26T06:13:49.948115Z","iopub.status.idle":"2024-04-26T06:13:49.968416Z","shell.execute_reply.started":"2024-04-26T06:13:49.948087Z","shell.execute_reply":"2024-04-26T06:13:49.967427Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model = train(net,2)  ","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:13:49.969808Z","iopub.execute_input":"2024-04-26T06:13:49.970127Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e6908628651416e8e33fd39a27cce0e"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 of 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/35008 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ad9afcb3f04536a3c34767cd7b7eb7"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_sizes['train']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_sizes['valid']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualization","metadata":{"execution":{"iopub.status.busy":"2021-08-28T06:30:10.260888Z","iopub.execute_input":"2021-08-28T06:30:10.26121Z","iopub.status.idle":"2021-08-28T06:30:10.282864Z","shell.execute_reply.started":"2021-08-28T06:30:10.26118Z","shell.execute_reply":"2021-08-28T06:30:10.281336Z"}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model_path = '../input/model-fc-diff/model.pth.tar'\nmodel = torch.load(model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val_model(model):\n    iou_list = []\n    for batch_index, batch in enumerate(tqdm(dataloaders['valid'])):\n        I1 = Variable(batch['I1'].float().to(device))\n        I2 = Variable(batch['I2'].float().to(device))\n        labels = Variable(batch['label'].int().to(device))\n        outputs = model(I1,I2)\n        iou_list.append(iou_pytorch(outputs.int().to('cpu'), labels.to('cpu')))\n    print(np.mean(iou_list)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_model(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(dataloaders['valid']))\nn = np.random.randint(BATCH_SIZE)\n_, preds = torch.max(model(batch['I1'].cuda(),batch['I2'].cuda()).data,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1,4)\nfig.suptitle('Random chip prediction')\nfig.tight_layout() \naxs[0].imshow(batch['I1'][n].squeeze())\naxs[1].imshow(batch['I2'][n].squeeze())\naxs[2].imshow(batch['label'][n].squeeze())\naxs[3].imshow(preds[n].cpu())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.stack((datasets['train'][0]['I1'],datasets['train'][0]['I1']),axis = 1).squeeze().shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(dataloaders['valid']))\nn = np.random.randint(BATCH_SIZE)\n_, preds = torch.max(model(batch['I1'].cuda(),batch['I2'].cuda()).data,1)\nfig, axs = plt.subplots(1,4)\nfig.suptitle('Random chip prediction')\nfig.tight_layout() \naxs[0].imshow(batch['I1'][n].squeeze())\naxs[1].imshow(batch['I2'][n].squeeze())\naxs[2].imshow(batch['label'][n].squeeze())\naxs[3].imshow(preds[n].cpu())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel.eval()\noutput_maps = []\nfor inputs in dataset:\n    inputs = inputs.to(device)\n    with torch.no_grad():\n        output = model(inputs)\n        output_maps.append(output.cpu().numpy())  \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Example: Convert output to geographic data\n\npoints = [Point(x, y) for y, row in enumerate(output_maps[0]) for x, val in enumerate(row) if val == 1]\ngeo_df = gpd.GeoDataFrame({'geometry': points}, crs=\"EPSG:4326\")  # Update CRS based on your data\n\n# Load a map for the background\nbase_map = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nfig, ax = plt.subplots(figsize=(10, 10))\nbase_map.plot(ax=ax, color='grey')\ngeo_df.plot(ax=ax, markersize=5, color='blue')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  `development_data` is a DataFrame where each row represents an area and columns represent years\ndevelopment_data['growth'] = development_data['year_5'] - development_data['year_1']\ndevelopment_data['growth_rate'] = development_data['growth'] / development_data['year_1']\n\n# Rank areas by growth rate\ndevelopment_data.sort_values(by='growth_rate', ascending=False, inplace=True)\ndevelopment_data['rank'] = range(1, len(development_data) + 1)\n\nprint(development_data[['area_name', 'growth_rate', 'rank']])\n","metadata":{},"execution_count":null,"outputs":[]}]}